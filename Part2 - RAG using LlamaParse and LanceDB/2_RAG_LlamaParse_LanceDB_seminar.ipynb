{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (with LanceDB and LlamaParse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = [\"Vafaei402723272.pdf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Parse PDF file using LlamaParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import os\n",
    "\n",
    "# set up parser\n",
    "parser = LlamaParse(result_type=\"text\", api_key=os.environ[\"LLAMA_PARSE_API_KEY\"])\n",
    "\n",
    "file_extractor = {\".pdf\": parser}\n",
    "\n",
    "data_for_parse = SimpleDirectoryReader(input_files=pdf_files, file_extractor=file_extractor)\n",
    "data_for_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents =data_for_parse.load_data()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=64,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_list = []\n",
    "page_number = 0\n",
    "last_doc = None\n",
    "for doc in documents:\n",
    "    if last_doc is None or last_doc != doc.metadata[\"file_name\"]:\n",
    "        page_number = 1\n",
    "        last_doc = doc.metadata[\"file_name\"]\n",
    "    else:\n",
    "        page_number += 1\n",
    "\n",
    "    texts = text_splitter.split_text(doc.text)\n",
    "    for text in texts:\n",
    "        item = {}\n",
    "        item[\"id_\"] = doc.id_\n",
    "        item[\"text\"] = text\n",
    "        item[\"metadata_file_name\"] = doc.metadata[\"file_name\"]\n",
    "        item[\"metadata_creation_date\"] = doc.metadata[\"creation_date\"]\n",
    "        item[\"metadata_pagenumber\"] = page_number\n",
    "        documents_list.append(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunks to Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(documents_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LanceDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "db = lancedb.connect(\".lancedb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lancedb.embeddings import get_registry\n",
    "embedding_model = get_registry().get(\"sentence-transformers\").create(name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the data model or schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You should put HF_TOKEN in the Notebook enviroment variables\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "\n",
    "class ChunksOfData(LanceModel):\n",
    "    id_: str\n",
    "    text: str = embedding_model.SourceField()\n",
    "    metadata_file_name: str\n",
    "    metadata_creation_date: str\n",
    "    metadata_pagenumber: int\n",
    "    vector: Vector(embedding_model.ndims()) = embedding_model.VectorField()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create table and add data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dict_batches(df: pd.DataFrame, batch_size: int = 128):\n",
    "    \"\"\"\n",
    "    Yields data from a DataFrame in batches of dictionaries.\n",
    "    Each batch is a list of dict, suitable for LanceDB ingestion.\n",
    "    \"\"\"\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = start_idx + batch_size\n",
    "        # Convert the batch of rows to a list of dict\n",
    "        batch_dicts = df.iloc[start_idx:end_idx].to_dict(orient=\"records\")\n",
    "        yield batch_dicts\n",
    "\n",
    "tbl = db.create_table(\n",
    "    \"seminar_pdf_data\",\n",
    "    data=df_to_dict_batches(df, batch_size=10),\n",
    "    schema=ChunksOfData,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying your table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = db.open_table(\"seminar_pdf_data\")\n",
    "query = \"مدل های زبانی بزرگ از نظر نوع ورودی و خروجی چگونه هستند؟\"\n",
    "#actual = table.search(query).limit(5).to_pydantic(Words)[0]\n",
    "res= tbl.search(query).limit(5).to_pandas()\n",
    "res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"کاربرد سیستم های صف چیست؟\"\n",
    "tbl.create_fts_index('text', use_tantivy=False,replace=True)\n",
    "tbl.search(query, query_type=\"hybrid\").limit(5).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "AVALAI_BASE_URL = \"https://api.avalai.ir/v1\"\n",
    "GPT_MODEL_NAME = \"gpt-4o-mini-2024-07-18\"\n",
    "\n",
    "gpt4o_chat = ChatOpenAI(model=GPT_MODEL_NAME,\n",
    "                        base_url=AVALAI_BASE_URL,\n",
    "                        api_key=os.environ[\"AVALAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List  # Add this line\n",
    "from langchain.llms.base import LLM\n",
    "from ollama import chat, ChatResponse\n",
    "from pydantic import BaseModel  # Pydantic's BaseModel for field definitions\n",
    "\n",
    "class OllamaLLM(LLM, BaseModel):\n",
    "    model_name: str\n",
    "    verbose: bool = False\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        if self.verbose:\n",
    "            print(f\"Sending prompt to {self.model_name}: {prompt}\")\n",
    "        response: ChatResponse = chat(\n",
    "            model=self.model_name,\n",
    "            messages=[{'role': 'user', 'content': prompt}]\n",
    "        )\n",
    "        return response.message.content\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ollama\"\n",
    "\n",
    "# Replace LlamaCpp with OllamaLLM\n",
    "model = OllamaLLM(model_name=\"llama3.2\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"کاربرد سیستم های صف چیست؟\"\n",
    "context_list = tbl.search(query, query_type=\"hybrid\").limit(5).to_list()\n",
    "context_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ''.join([f\"{c['text']}\\n\\n\" for c in context_list])\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Answer user query based on the given context.\"\n",
    "user_prompt = f\"Question:\\n{query}\\nContext:\\n{context}\"\n",
    "print(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(system_prompt),\n",
    "    HumanMessage(user_prompt),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer user query based on the given context.\n",
    "\n",
    "    Question:{Question}\n",
    "\n",
    "    Context:\n",
    "    {Context}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "response2  = chain.invoke({\n",
    "    \"Question\": query,\n",
    "    \"Context\": context\n",
    "})\n",
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = gpt4o_chat.invoke(messages)\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='llama3.2', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': query,\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"کاربرد سیستم های صف چیست؟\"\n",
    "response = gpt4o_chat.invoke(query)\n",
    "response.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
